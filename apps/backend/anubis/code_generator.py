"""
CodeGenerator Module
Interfaces with Google ADK to generate code from different AI models.
"""

import os
import time
import threading
import queue
from typing import Dict, List, Optional, Generator
from google import genai
from google.genai import types


class CodeGenerator:
    """Generates code using various AI models via Google ADK."""

    def __init__(self, api_key: Optional[str] = None, max_retries: int = 3):
        """
        Initialize the CodeGenerator.

        Args:
            api_key: Google API key. If None, reads from GOOGLE_API_KEY env var.
            max_retries: Maximum number of retry attempts for failed API calls.
        """
        self.api_key = api_key or os.environ.get('GOOGLE_API_KEY')
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY must be set in environment or passed as parameter")

        self.client = genai.Client(api_key=self.api_key)
        self.max_retries = max_retries

    def generate_code(
        self,
        prompt: str,
        model: str,
        metrics_priority: Optional[List[str]] = None,
        timeout: int = 30
    ) -> Dict[str, any]:
        """
        Generate code from a single AI model.

        Args:
            prompt: The coding task description.
            model: The AI model identifier.
            metrics_priority: Optional list of metrics in priority order for code generation focus.
            timeout: Maximum time in seconds to wait for response.

        Returns:
            Dictionary containing:
                - generated_code: The code generated by the model
                - execution_time_ms: Time taken to generate the code
                - success: Whether generation was successful
                - error: Error message if failed
        """
        start_time = time.time()

        for attempt in range(self.max_retries):
            try:
                # Create a structured prompt for code generation
                formatted_prompt = self._build_prompt(prompt, metrics_priority)

                response = self.client.models.generate_content(
                    model=model,
                    contents=formatted_prompt
                )

                execution_time_ms = int((time.time() - start_time) * 1000)

                # Extract code from response
                generated_code = self._extract_code(response.text)

                return {
                    'generated_code': generated_code,
                    'execution_time_ms': execution_time_ms,
                    'success': True,
                    'error': None,
                    'model': model
                }

            except Exception as e:
                if attempt == self.max_retries - 1:
                    # Last attempt failed
                    execution_time_ms = int((time.time() - start_time) * 1000)
                    return {
                        'generated_code': '',
                        'execution_time_ms': execution_time_ms,
                        'success': False,
                        'error': str(e),
                        'model': model
                    }
                # Wait before retry (exponential backoff)
                time.sleep(2 ** attempt)

    def generate_code_multi_models(
        self,
        prompt: str,
        models: List[str],
        metrics_priority: Optional[List[str]] = None
    ) -> List[Dict[str, any]]:
        """
        Generate code from multiple AI models.

        Args:
            prompt: The coding task description.
            models: List of AI model identifiers.
            metrics_priority: Optional list of metrics in priority order for code generation focus.

        Returns:
            List of generation results, one per model.
        """
        results = []
        for model in models:
            result = self.generate_code(prompt, model, metrics_priority)
            results.append(result)

        return results

    def _build_prompt(self, prompt: str, metrics_priority: Optional[List[str]] = None) -> str:
        """
        Build the formatted prompt for code generation with metric priorities.

        Args:
            prompt: The coding task description.
            metrics_priority: Optional list of metrics in priority order.

        Returns:
            Formatted prompt string
        """
        base_prompt = f"""Generate Python code for the following task:

Task: {prompt}

Requirements:
- Write clean, well-documented Python code
- Include docstrings and comments where appropriate
- Follow PEP 8 style guidelines
- Use descriptive variable names
- Only return the code, no explanations"""

        if metrics_priority:
            # Map metric names to user-friendly descriptions
            metric_descriptions = {
                'readability': 'Code readability (clear variable names, good structure, helpful comments)',
                'consistency': 'Code consistency (uniform naming conventions, consistent style)',
                'time_complexity': 'Time complexity optimization (efficient algorithms, best Big O complexity)',
                'code_documentation': 'Code documentation (comprehensive docstrings, inline comments)',
                'external_dependencies': 'Minimal external dependencies (prefer standard library)'
            }

            priority_section = "\n\nEVALUATION PRIORITIES (in order of importance):"
            for i, metric in enumerate(metrics_priority, 1):
                if metric in metric_descriptions:
                    priority_section += f"\n{i}. {metric_descriptions[metric]}"

            priority_section += "\n\nIMPORTANT: The code will be evaluated with weighted scoring where higher-ranked criteria have exponentially greater impact on the final score. Focus especially on the top priorities."

            base_prompt += priority_section

        base_prompt += "\n\nCode:"
        return base_prompt

    def generate_code_stream(
        self,
        prompt: str,
        model: str,
        metrics_priority: Optional[List[str]] = None
    ):
        """
        Generate code from a single AI model with streaming support.

        Args:
            prompt: The coding task description.
            model: The AI model identifier.
            metrics_priority: Optional list of metrics in priority order for code generation focus.

        Yields:
            Dictionary containing:
                - chunk: The text chunk from the model
                - is_complete: Whether generation is complete
                - generated_code: Full code (only when is_complete=True)
                - execution_time_ms: Time taken (only when is_complete=True)
                - success: Whether generation was successful
                - error: Error message if failed
        """
        start_time = time.time()
        accumulated_text = ""

        try:
            # Create a structured prompt for code generation
            formatted_prompt = self._build_prompt(prompt, metrics_priority)

            # Use streaming API
            response_stream = self.client.models.generate_content_stream(
                model=model,
                contents=formatted_prompt
            )

            # Stream chunks as they arrive
            for chunk in response_stream:
                if chunk.text:
                    accumulated_text += chunk.text
                    yield {
                        'chunk': chunk.text,
                        'is_complete': False,
                        'success': True,
                        'error': None,
                        'model': model
                    }

            # Generation complete
            execution_time_ms = int((time.time() - start_time) * 1000)
            generated_code = self._extract_code(accumulated_text)

            yield {
                'chunk': '',
                'is_complete': True,
                'generated_code': generated_code,
                'execution_time_ms': execution_time_ms,
                'success': True,
                'error': None,
                'model': model
            }

        except Exception as e:
            execution_time_ms = int((time.time() - start_time) * 1000)
            yield {
                'chunk': '',
                'is_complete': True,
                'generated_code': '',
                'execution_time_ms': execution_time_ms,
                'success': False,
                'error': str(e),
                'model': model
            }

    def generate_code_multi_models_stream(
        self,
        prompt: str,
        models: List[str],
        metrics_priority: Optional[List[str]] = None
    ) -> Generator[Dict[str, any], None, None]:
        """
        Generate code from multiple AI models in parallel with streaming support.

        Args:
            prompt: The coding task description.
            models: List of AI model identifiers.
            metrics_priority: Optional list of metrics in priority order for code generation focus.

        Yields:
            Dictionary containing events from all models as they arrive:
                - chunk: The text chunk from a model
                - model: Which model generated this chunk
                - is_complete: Whether this model's generation is complete
                - generated_code: Full code (only when is_complete=True)
                - execution_time_ms: Time taken (only when is_complete=True)
                - success: Whether generation was successful
                - error: Error message if failed
        """
        # Shared queue for collecting chunks from all threads
        chunk_queue = queue.Queue()

        # Track completion status
        completed_models = set()
        total_models = len(models)

        def stream_worker(model: str):
            """Worker thread that streams code generation for a single model."""
            try:
                for chunk_data in self.generate_code_stream(prompt, model, metrics_priority):
                    # Put chunk into shared queue
                    chunk_queue.put(chunk_data)

                    # If this model completed, mark it
                    if chunk_data.get('is_complete'):
                        completed_models.add(model)
            except Exception as e:
                # Put error into queue
                chunk_queue.put({
                    'chunk': '',
                    'is_complete': True,
                    'generated_code': '',
                    'execution_time_ms': 0,
                    'success': False,
                    'error': str(e),
                    'model': model
                })
                completed_models.add(model)

        # Start all model threads
        threads = []
        for model in models:
            thread = threading.Thread(target=stream_worker, args=(model,))
            thread.daemon = True
            thread.start()
            threads.append(thread)

        # Yield chunks as they arrive from any model
        while len(completed_models) < total_models:
            try:
                # Get chunk with timeout to prevent hanging
                chunk_data = chunk_queue.get(timeout=1.0)
                yield chunk_data
            except queue.Empty:
                # No chunks available, continue waiting
                continue

        # Make sure all threads have finished
        for thread in threads:
            thread.join(timeout=1.0)

    def _extract_code(self, response_text: str) -> str:
        """
        Extract code from model response, removing markdown formatting.

        Args:
            response_text: Raw response from the model.

        Returns:
            Cleaned code string.
        """
        # Remove markdown code blocks
        if '```python' in response_text:
            # Extract code between ```python and ```
            start = response_text.find('```python') + len('```python')
            end = response_text.find('```', start)
            if end != -1:
                return response_text[start:end].strip()
        elif '```' in response_text:
            # Extract code between ``` and ```
            start = response_text.find('```') + len('```')
            end = response_text.find('```', start)
            if end != -1:
                return response_text[start:end].strip()

        # Return as-is if no markdown formatting found
        return response_text.strip()
